{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "497b7a31-a2d2-4ddb-8e99-bc08b7d2f202",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dspy\n",
    "import csv\n",
    "# import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a79c68f-551d-4bb8-a421-2fd5c146c683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ujson\n",
    "import faiss\n",
    "from langchain_community.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5194a39e-d66f-4793-b705-3f00a1d11153",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_huggingface import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f10a4b-1d94-45ae-b6bd-eed1b92b57ec",
   "metadata": {},
   "source": [
    "<!-- ```mermaid\n",
    "graph LR;\n",
    "    A--> B & C & D;\n",
    "    B--> A & E;\n",
    "    C--> A & E;\n",
    "    D--> A & E;\n",
    "    E--> B & C & D; -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab4d7d0-f3af-4414-aad0-0f6c37e2163b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "187a3212-329a-4e6d-a9d4-e2d43c940f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #generate llm object\n",
    "# local_config = {\n",
    "#     \"api_base\": \"http://localhost:11434/v1\",  # 注意需加/v1路徑\n",
    "#     \"api_key\": \"NULL\",  # 特殊標記用於跳過驗證\n",
    "#     \"model\": \"deepseek-r1:7b\",\n",
    "#     \"custom_llm_provider\":\"deepseek\"\n",
    "# }\n",
    "\n",
    "# dspy.configure(\n",
    "#     lm=dspy.LM(\n",
    "#         **local_config\n",
    "#     )\n",
    "# )\n",
    "# # 測試問答\n",
    "# qa = dspy.Predict('question -> answer')\n",
    "# response = qa(question=\"中國唐朝有幾任皇帝?\")\n",
    "# print(f\"模型回答：{response.answer}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1588eef-a7c5-4d96-8ec4-291c390feb18",
   "metadata": {},
   "source": [
    "### ref: https://dspy.ai/tutorials/rag/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "acb39963-03b4-4671-bd73-4de3f568f49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def InitializeLLM():\n",
    "#     local_config = {\n",
    "#         \"api_base\": \"http://localhost:11434/v1\",  # 注意需加/v1路徑\n",
    "#         \"api_key\": \"NULL\",  # 特殊標記用於跳過驗證\n",
    "#         \"model\": \"deepseek-r1:7b\",\n",
    "#         \"custom_llm_provider\":\"deepseek\"\n",
    "#     }\n",
    "#     dspy.configure(\n",
    "#         lm=dspy.LM(\n",
    "#             **local_config\n",
    "#         )\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ec1cb72-8185-497e-8a1d-7c124bfdea1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def InitializeLLM():\n",
    "    lm = dspy.LM('ollama_chat/llama3.2', api_base='http://localhost:11434', api_key='')\n",
    "    dspy.configure(lm=lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b116f5e-ab84-4bc3-a59e-7774d4298995",
   "metadata": {},
   "outputs": [],
   "source": [
    "InitializeLLM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bfdceb93-ca65-4a46-9ead-be92e9d5ed44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_predict(queryStr:str=None):\n",
    "    if queryStr == None:\n",
    "        raise ValueError(\"query string is none, please input query string.\")\n",
    "    promptPatternStr = \"question -> answer\"\n",
    "    qa = dspy.Predict(promptPatternStr);\n",
    "    response = qa(question=queryStr);\n",
    "    print(f\"llm:{response.answer}\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca617257-df47-4b54-b0a4-78e9e65e4bf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llm:Today is [current date].\n"
     ]
    }
   ],
   "source": [
    "llm_predict(\"what day is today\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d61cc9fa-9f6d-421b-bce0-e4d720fbf23c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[34m[2025-02-24T16:41:15.044820]\u001b[0m\n",
      "\n",
      "\u001b[31mSystem message:\u001b[0m\n",
      "\n",
      "Your input fields are:\n",
      "1. `question` (str)\n",
      "\n",
      "Your output fields are:\n",
      "1. `answer` (str)\n",
      "\n",
      "All interactions will be structured in the following way, with the appropriate values filled in.\n",
      "\n",
      "[[ ## question ## ]]\n",
      "{question}\n",
      "\n",
      "[[ ## answer ## ]]\n",
      "{answer}\n",
      "\n",
      "[[ ## completed ## ]]\n",
      "\n",
      "In adhering to this structure, your objective is: \n",
      "        Given the fields `question`, produce the fields `answer`.\n",
      "\n",
      "\n",
      "\u001b[31mUser message:\u001b[0m\n",
      "\n",
      "[[ ## question ## ]]\n",
      "what day is today\n",
      "\n",
      "Respond with the corresponding output fields, starting with the field `[[ ## answer ## ]]`, and then ending with the marker for `[[ ## completed ## ]]`.\n",
      "\n",
      "\n",
      "\u001b[31mResponse:\u001b[0m\n",
      "\n",
      "\u001b[32m[[ ## answer ## ]]\n",
      "Today is [current date].\n",
      "\n",
      "[[ ## completed ## ]]\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dspy.inspect_history(n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c80803a-54e6-4738-8092-db1f79435c3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(\n",
       "    reasoning='{The reasoning field is where you would provide a detailed thought process or explanation based on the input question. It should be structured in a way that clearly shows how you arrived at your answer.}',\n",
       "    response='{The response field is where you provide a concise and direct answer to the question, typically one or two sentences long. It should be clear and to the point, summarizing the key points from your reasoning.}'\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cot = dspy.ChainOfThought('question -> response')\n",
    "cot(question=\"should curly braces appear on their own line?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fc710b0a-d387-4d62-a975-f8d04c485675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"./data/ragqa_arena_tech_examples.jsonl\") as f:\n",
    "#     data = [ujson.loads(line) for line in f]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68fb6053-ae91-4c26-8e51-f5b13950c7d6",
   "metadata": {},
   "source": [
    "### Basic RAG\n",
    "- dspy.Embedder: https://dspy.ai/api/models/Embedder/\n",
    "- multihop Search: https://dspy.ai/tutorials/multihop_search/?h=search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa443466-0f70-4946-a899-84ca853a3629",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/ragqa_arena_tech_corpus.jsonl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m max_characters \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m6000\u001b[39m  \u001b[38;5;66;03m# for truncating >99th percentile of documents\u001b[39;00m\n\u001b[1;32m      2\u001b[0m topk_docs_to_retrieve \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m  \u001b[38;5;66;03m# number of documents to retrieve per search query\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../data/ragqa_arena_tech_corpus.jsonl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      5\u001b[0m     corpus \u001b[38;5;241m=\u001b[39m [ujson\u001b[38;5;241m.\u001b[39mloads(line)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m][:max_characters] \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m f]\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(corpus)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m documents. Will encode them below.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/llmenv/lib/python3.11/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/ragqa_arena_tech_corpus.jsonl'"
     ]
    }
   ],
   "source": [
    "max_characters = 6000  # for truncating >99th percentile of documents\n",
    "topk_docs_to_retrieve = 5  # number of documents to retrieve per search query\n",
    "\n",
    "with open(\"../data/ragqa_arena_tech_corpus.jsonl\") as f:\n",
    "    corpus = [ujson.loads(line)['text'][:max_characters] for line in f]\n",
    "    print(f\"Loaded {len(corpus)} documents. Will encode them below.\")\n",
    "model = SentenceTransformer(\"sentence-transformers/static-retrieval-mrl-en-v1\", device=\"cpu\")\n",
    "embedder = dspy.Embedder(model=model.encode, dimensions=512)\n",
    "search = dspy.retrievers.Embeddings(embedder=embedder, corpus=corpus, k=topk_docs_to_retrieve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "af952782-3d44-4a60-8157-5550b1fdd15b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You can use StartSound.PrefPane which basically just sets the volume to 0 when you shutdown and then turns it back up after login.'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d4ed7ae-2576-4533-a0f5-12996c8be92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAG(dspy.Module):\n",
    "    def __init__(self):\n",
    "        self.respond=dspy.ChainOfThought('context, question -> answer');\n",
    "\n",
    "    def forward(self,question):\n",
    "        context = search(question).passages\n",
    "        return self.respond(context=context, question=question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c4b10937-7bfc-41df-8888-e8e8a1964e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag = RAG();\n",
    "# print(rag(question=\"what are high memory and low memory on linux?\"));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "225ae1f3-71b6-4287-a472-6d73d5bf75cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dspy.inspect_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0ed80c35-5999-44f4-8577-c3f55729aaa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGChainOfThought(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Define the chain of thought predictor\n",
    "        self.qa_chain = dspy.ChainOfThought('context, question -> answer')\n",
    "        \n",
    "        # Define retrieval module\n",
    "        self.retrieve = dspy.Retrieve(k=3)  # Retrieve top 3 relevant passages\n",
    "    \n",
    "    def forward(self, question):\n",
    "        # Retrieve relevant contexts\n",
    "        retrieved_contexts = self.retrieve(question).passages\n",
    "        \n",
    "        # Combine contexts\n",
    "        context = \" \".join(retrieved_contexts)\n",
    "        \n",
    "        # Generate answer using chain of thought reasoning\n",
    "        prediction = self.qa_chain(context=context, question=question)\n",
    "        \n",
    "        return prediction.answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7449b6d1-86ca-4885-a01d-de8b36cb28a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_embedding_and_query(idx_path=None, local_db_path=None):\n",
    "    load_index(idx_path=idx_path);\n",
    "    load_local_db(local_db_path=local_db_path);\n",
    "\n",
    "def load_index(idx_path=None):\n",
    "    #載入預訓練的FAISS索引\n",
    "    try:\n",
    "        index = faiss.read_index(idx_path)\n",
    "        print(f\"成功載入FAISS索引，包含 {index.ntotal} 個向量\")\n",
    "        return index\n",
    "    except Exception as e:\n",
    "        print(f\"索引載入失敗: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def load_local_db(local_db_path=None):\n",
    "    #載入完整的向量資料庫\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=\"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\"\n",
    "    )\n",
    "    try:\n",
    "        db = FAISS.load_local(\n",
    "            folder_path=local_db_path,\n",
    "            embeddings=embeddings,\n",
    "            allow_dangerous_deserialization=True  # 必要安全參數\n",
    "        )\n",
    "        print(f\"載入成功，共 {db.index.ntotal} 筆技術問答\")\n",
    "        return db\n",
    "    except Exception as e:\n",
    "        print(f\"向量庫載入異常: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cc646dfd-da8f-47e3-bffb-9a46ae99f6ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "成功載入FAISS索引，包含 145 個向量\n",
      "載入成功，共 145 筆技術問答\n"
     ]
    }
   ],
   "source": [
    "_idx_path = \"./db/qa_index.faiss\"\n",
    "_local_db = \"./db/tech_support_faiss/\"\n",
    "read_embedding_and_query(idx_path=_idx_path,local_db_path=_local_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d86b97-3b90-4577-872b-41d546e038b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
